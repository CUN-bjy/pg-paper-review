# rl-paper-review



## Policy Gradient

### (1) Vanila PG(Sutton)

[Policy gradient methods for reinforcement learning with function approximation]

Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour,1994

[review_page](./reviews/Sutton_PG.md)	|   [paper link](http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)

<br/>

### (2) DPG

[Deterministic policy gradient algorithms]

Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., & Riedmiller, M. (2014).

[review_page](./reviews/DPG.md)  |  [paper_link](http://proceedings.mlr.press/v32/silver14.pdf)

<br/>

### (3) DDPG

[Continuous control with deep reinforcement learning]

Timothy P. Lillicrap∗ , Jonathan J. Hunt∗ , Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver & Daan Wierstra (2016)

[review_page](./reviews/DDPG.md)  |  [paper_link](https://arxiv.org/pdf/1509.02971.pdf)  | [implementation](https://github.com/CUN-bjy/walkyto-ddpg)

<br/>

### (4) NPG

[A natural policy gradient]

Sham Kakade(2002)

[review_page](./reviews/NPG.md)  |  [paper_link](https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf)

<br/>

### (5) TRPO

[Trust region policy optimization]

John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, Pieter Abbeel (2015)

[review_page](./reviews/TRPO.md)| [paper_link](https://arxiv.org/pdf/1502.05477.pdf)

<br/>

### (6) GAE

[High-Dimensional Continuous Control Using Generalized Advantage Estimation]

John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel(2016)

[review_page](./reviews/GAE.md)| [paper_link](https://arxiv.org/pdf/1506.02438.pdf)

<br/>

### (7) PPO

[Proximal policy optimization algorithms]

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov(2017)

[review_page](./reviews/PPO.md)| [paper_link](https://arxiv.org/pdf/1707.06347.pdf) | [implementation](https://github.com/CUN-bjy/gym-ppo-keras)

</br>

### (8) ACER

[ACER: Sample Efficient Actor-Critic With Experience Replay]

</br>

### (9) ACKTR

[Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation]

<br/>

### (10) SAC
[Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor]

</br>

### (11) TD3

[Addressing Function Approximation Error in Actor-Critic Methods]

review_page | [paper_link](https://arxiv.org/pdf/1802.09477.pdf)

</br>

### (12) C51(distributional aspect)

[A Distributional Perspective on Reinforcement Learning]

review_page | [paper_link](https://arxiv.org/pdf/1707.06887.pdf)

</br>

## Exploration

### (1) PER

[Prioritized Experience Replay]

Tom Schaul, John Quan, Ioannis Antonoglou and David Silver, Google DeepMind(2015)

[review_page](./reviews/PER.md) | [paper_link](https://arxiv.org/pdf/1511.05952.pdf) 

</br>

### (2) HER

[Hindsight Experience Replay, Marcin Andrychowicz]

Marcin Andrychowicz∗ , Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel , Wojciech Zaremba ,OpenAI(2018)

review_page | [paper_link](https://arxiv.org/pdf/1707.01495.pdf)

</br>

## Distributed RL

### (1) Ape-X

[Distributed Prioritized Experience Replay]

</br>

### (2) IMPALA

[IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-learner Architectures]

</br>

## Hierarchical RL

### (1) Option-Critic

[The Option-Critic Architecture]

</br>

### (2) MLSH

[Meta Learning Shared Hierarchies]

</br>

## Meta RL / Multi-task RL

</br>

---

### Reference

[Key Papers in Deep RL](https://spinningup.openai.com/en/latest/spinningup/keypapers.html#id106)

[PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)

[utilForever/rl-paper-study](https://github.com/utilForever/rl-paper-study)