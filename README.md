# rl-paper-review

### Paper Review List

- [x] [1. Sutton_PG](#1-sutton_pg)
- [x] [2. DPG](#2-dpg)
- [x] [3. DDPG](#3-ddpg)
- [x] [4. NPG](#4-npg)
- [ ] [5. TRPO](#5-trpo)
- [ ] [6. GAE](#6-gae)
- [ ] [7. PPO](#7-ppo)
- [ ] [8. TD3](#8-td3)
- [ ] [9. SAC](#9-sac)

<br/>

### 1. Sutton_PG

[Policy gradient methods for reinforcement learning with function approximation]

Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour,1994

[review_page](./reviews/Sutton_PG.md)	|   [paper link](http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)

<br/>

### 2. DPG

[Deterministic policy gradient algorithms]

Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., & Riedmiller, M. (2014).

[review_page](./reviews/DPG.md)  |  [paper_link](http://proceedings.mlr.press/v32/silver14.pdf)

<br/>

### 3. DDPG

[Continuous control with deep reinforcement learning]

Timothy P. Lillicrap∗ , Jonathan J. Hunt∗ , Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver & Daan Wierstra (2016)

[review_page](./reviews/DDPG.md)  |  [paper_link](https://arxiv.org/pdf/1509.02971.pdf)  | [implementation](https://github.com/CUN-bjy/walkyto-ddpg)

<br/>

### 4. NPG

[A natural policy gradient]

Sham Kakade(2002)

[review_page](./reviews/NPG.md)  |  [paper_link](https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf)

<br/>

### 5. TRPO

[Trust region policy optimization]

John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, Pieter Abbeel (2015)

[review_page](./reviews/TRPO.md)| [paper_link](https://arxiv.org/pdf/1502.05477.pdf)

<br/>

### 6. GAE

[High-Dimensional Continuous Control Using Generalized Advantage Estimation]

John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel(2016)

[review_page](./reviews/GAE.md)| [paper_link](https://arxiv.org/pdf/1506.02438.pdf)

<br/>

### 7. PPO

[Proximal policy optimization algorithms]

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov

[review_page](./reviews/GAE.md)| [paper_link](https://arxiv.org/pdf/1707.06347.pdf) | [implementation](https://github.com/CUN-bjy/gym-ppo-keras)

<br/>

### 8. TD3
[Addressing Function Approximation Error in Actor-Critic Methods]

<br/>

### 9. SAC
[Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor]

</br>

---

### Reference
[PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)