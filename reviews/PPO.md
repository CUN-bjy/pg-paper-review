[HOME](../README.md)

### PPO

[Proximal policy optimization algorithms]

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov

[paper_link](https://arxiv.org/pdf/1707.06347.pdf) | [implementation](https://github.com/CUN-bjy/gym-ppo-keras)

<br/>

### [Scheme]

TRPO에 이은 John Schulman의 새로운 Policy Optimization 알고리즘이며, 꾸준히 자신이 세운 SOTA 기록들을 갱신하며 넘사벽의 존재가 되어버렸다.

PPO가 나온지 3년이 지난 2020년 현재. TD3, SAC 등 좋은 알고리즘들이 많이 등장했지만, 쉬운 구현과 상징성의 이유로 아직까지 제일 유명한 알고리즘으로서 꼽힌다.

</br>

해당 논문은 [TRPO](./TRPO.md)와 관련이 매우 깊다. 





### [Abstract]



### [Background: Trust Region Methods]



### [Clipped Surrogate Objective]



### [Adaptive KL Penalty Coefficient]



### [PPO]



### [Experiments]



### [Conclusion]